<!DOCTYPE html>
<html><head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta name="description" content="Eric Medvet, Associate Professor of Computer Engineering at the University of Trieste, Italy" />
  <meta name="author" content="Eric Medvet" />
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx" crossorigin="anonymous"></script>  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous"/>
  <link rel="stylesheet" href="/scss/basic.css" />
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZW00T23DHQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag("js", new Date());
    gtag("config", "G-ZW00T23DHQ");
  </script>
  <title>Learning a Formula of Interpretability to Learn Interpretable Formulas</title>
</head>
<body>
    <div class="container">
      <div class="inner-container border rounded"><nav class="navbar navbar-expand-lg navbar-light bg-light">
  <a class="navbar-brand" href="https://ericmedvet.github.io/">
      <span class="initial">E</span><span>ric</span>
      <span class="initial">M</span><span>edvet</span></a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
    <div class="navbar-nav">
      <a class="nav-link active" href="/publications/">Publications</a>
      <a class="nav-link " href="/teaching/">Teaching</a>
      <a class="nav-link " href="/contact/">Contact me</a>
    </div>
  </div>
</nav>
<div id="content">
<div class="container">
  <h1>Learning a Formula of Interpretability to Learn Interpretable Formulas</h1>

  <div class="publication-data">
    <h5>Type:</h5>
    <p><span class="badge-pub-type badge badge-pill badge-pub-type-secondary">Conf</span>
</p>

    <h5>Authors:</h5>
    <p>
        <span >Marco Virgolin</span>, 
        <span >Andrea De Lorenzo</span>, 
        <span class="this-author">Eric Medvet</span>, 
        <span >Francesca Randone</span></p>

    <h5>In:</h5>
    <p>16th <strong>International Conference on Parallel Problem Solving from Nature (PPSN)</strong>, held in Leiden (Netherlands)</p>

    <h5>Year:</h5>
    <p>2020</p>

    

    <h5>Links and material:</h5>
    <ul>
    <li><a href="https://scholar.google.com/scholar?q=learning&#43;a&#43;formula&#43;of&#43;interpretability&#43;to&#43;learn&#43;interpretable&#43;formulas">Google Scholar</a></li>
    <li><a href="https://link.springer.com/chapter/10.1007%2F978-3-030-58115-2_6">Publisher version</a></li>
    <li>DOI: <a href="https://doi.org/10.1007/978-3-030-58115-2_6">10.1007/978-3-030-58115-2_6</a></li>
    <li><a href="https://drive.google.com/uc?export=download&amp;id=1qvG7bu9adWkb830l77BrEfvMCGlM1MCZ">Full text</a></li>
    
    
    </ul>

  </div>

  <h2 id="abstract" class="wl">
  <span>Abstract</span>
  <a href="#abstract" aria-label="Anchor">#</a>
  <a href="#content">â†°</a>
</h2>

<p>Many risk-sensitive applications require Machine Learning (ML) models to be interpretable. Attempts to obtain interpretable models typically rely on tuning, by trial-and-error, hyper-parameters of model complexity that are only loosely related to interpretability. We show that it is instead possible to take a meta-learning approach: an ML model of non-trivial Proxies of Human Interpretability (PHIs) can be learned from human feedback, then this model can be incorporated within an ML training process to directly optimize for interpretability. We show this for evolutionary symbolic regression. We first design and distribute a survey finalized at finding a link between features of mathematical formulas and two established PHIs, simulatability and decomposability. Next, we use the resulting dataset to learn an ML model of interpretability. Lastly, we query this model to estimate the interpretability of evolving solutions within bi-objective genetic programming. We perform experiments on five synthetic and eight real-world symbolic regression problems, comparing to the traditional use of solution size minimization. The results show that the use of our model leads to formulas that are, for a same level of accuracy-interpretability trade-off, either significantly more or equally accurate. Moreover, the formulas are also arguably more interpretable. Given the very positive results, we believe that our approach represents an important stepping stone for the design of next-generation interpretable (evolutionary) ML algorithms.</p>

</div>

        </div>
      </div>
<div id="footer">
  <div class="text-center">
    Copyright by Eric Medvet
  </div>
</div>

</div>
  </body>
</html>
