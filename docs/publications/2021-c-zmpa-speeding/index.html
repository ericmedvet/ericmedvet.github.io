<!DOCTYPE html>
<html><head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta name="description" content="Eric Medvet, Associate Professor of Computer Engineering at the University of Trieste, Italy" />
  <meta name="author" content="Eric Medvet" />
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx" crossorigin="anonymous"></script>  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous"/>
  <link rel="stylesheet" href="/scss/basic.css" />
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZW00T23DHQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag("js", new Date());
    gtag("config", "G-ZW00T23DHQ");
  </script>
  <title>Speeding-up pruning for Artificial Neural Networks: Introducing Accelerated Iterative Magnitude Pruning</title>
</head>
<body>
    <div class="container">
      <div class="inner-container border rounded"><nav class="navbar navbar-expand-lg navbar-light bg-light">
  <a class="navbar-brand" href="https://ericmedvet.github.io/">
      <span class="initial">E</span><span>ric</span>
      <span class="initial">M</span><span>edvet</span></a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
    <div class="navbar-nav">
      <a class="nav-link active" href="/publications/">Publications</a>
      <a class="nav-link " href="/teaching/">Teaching</a>
      <a class="nav-link " href="/contact/">Contact me</a>
    </div>
  </div>
</nav>
<div id="content">
<div class="container">
  <h1>Speeding-up pruning for Artificial Neural Networks: Introducing Accelerated Iterative Magnitude Pruning</h1>

  <div class="publication-data">
    <h5>Type:</h5>
    <p><span class="badge-pub-type badge badge-pill badge-pub-type-secondary">Conf</span>
</p>

    <h5>Authors:</h5>
    <p>
        <span >Marco Zullich</span>, 
        <span class="this-author">Eric Medvet</span>, 
        <span >Felice Andrea Pellegrino</span>, 
        <span >Alessio Ansuini</span></p>

    <h5>In:</h5>
    <p>25th <strong>International Conference on Pattern Recognition (ICPR)</strong>, held in Milano (Italy)</p>

    <h5>Year:</h5>
    <p>2021</p>

    
      <h5>Notes:</h5>
      <p>To appear</p>
    

    <h5>Links and material:</h5>
    <ul>
    <li><a href="https://scholar.google.com/scholar?q=speeding&#43;up&#43;pruning&#43;for&#43;artificial&#43;neural&#43;networks&#43;&#43;introducing&#43;accelerated&#43;iterative&#43;magnitude&#43;pruning">Google Scholar</a></li>
    
    
    
    
    
    </ul>

  </div>

  <h2 id="abstract" class="wl">
  <span>Abstract</span>
  <a href="#abstract" aria-label="Anchor">#</a>
  <a href="#content">â†°</a>
</h2>

<p>In recent years, Artificial Neural Networks (ANNs) pruning has become the focal point of many researches, due to the extreme overparametrization of such models. This has urged the scientific world to investigate methods for the simplification of the structure of weights in ANNs, mainly in an effort to reduce time for both training and inference. Frankle and Carbin, and later Renda, Frankle, and Carbin introduced and refined an iterative pruning method which is able to effectively prune the network of a great portion of its parameters with little to no loss in performance. On the downside, this method requires a large amount of time for its application, since, for each iteration, the network has to be trained for (almost) the same amount of epochs of the unpruned network. In this work, we show that, for a limited setting, if targeting high overall sparsity rates, this time can be effectively reduced for each iteration, save for the last one, by more than 50%, while yielding a final product (i.e., final pruned network) whose performance is comparable to the ANN obtained using the existing method.</p>

</div>

        </div>
      </div>
<div id="footer">
  <div class="text-center">
    Copyright by Eric Medvet
  </div>
</div>

</div>
  </body>
</html>
