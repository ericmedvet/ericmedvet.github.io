<!DOCTYPE html>
<html><head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta name="description" content="Eric Medvet, Associate Professor of Computer Engineering at the University of Trieste, Italy" />
  <meta name="author" content="Eric Medvet" />
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx" crossorigin="anonymous"></script>  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous"/>
  <link rel="stylesheet" href="/scss/basic.css" />
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZW00T23DHQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag("js", new Date());
    gtag("config", "G-ZW00T23DHQ");
  </script>
  <title>An analysis of the ingredients for learning interpretable symbolic regression models with human-in-the-loop and genetic programming</title>
</head>
<body>
    <div class="container">
      <div class="inner-container border rounded"><nav class="navbar navbar-expand-lg navbar-light bg-light">
  <a class="navbar-brand" href="https://ericmedvet.github.io/">
      <span class="initial">E</span><span>ric</span>
      <span class="initial">M</span><span>edvet</span></a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
    <div class="navbar-nav">
      <a class="nav-link active" href="/publications/">Publications</a>
      <a class="nav-link " href="/teaching/">Teaching</a>
      <a class="nav-link " href="/contact/">Contact me</a>
    </div>
  </div>
</nav>
<div id="content">
<div class="container">
  <h1>An analysis of the ingredients for learning interpretable symbolic regression models with human-in-the-loop and genetic programming</h1>

  <div class="publication-data">
    <h5>Type:</h5>
    <p><span class="badge-pub-type badge badge-pill badge-pub-type-primary">Jour</span>
</p>

    <h5>Authors:</h5>
    <p>
        <span >Giorgia Nadizar</span>, 
        <span >Luigi Rovito</span>, 
        <span >Andrea De Lorenzo</span>, 
        <span class="this-author">Eric Medvet</span>, 
        <span >Marco Virgolin</span></p>

    <h5>In:</h5>
    <p><strong>ACM Transactions on Evolutionary Learning and Optimization (TELO)</strong><br>(rank <strong title="According to Scimagojr">Q1</strong> in Computer Science (Miscellaneous))</p>

    <h5>Year:</h5>
    <p>2024</p>

    

    <h5>Links and material:</h5>
    <ul>
    <li><a href="https://scholar.google.com/scholar?q=an&#43;analysis&#43;of&#43;the&#43;ingredients&#43;for&#43;learning&#43;interpretable&#43;symbolic&#43;regression&#43;models&#43;with&#43;human&#43;in&#43;the&#43;loop&#43;and&#43;genetic&#43;programming">Google Scholar</a></li>
    <li><a href="https://dl.acm.org/doi/10.1145/3643688">Publisher version</a></li>
    <li>DOI: <a href="https://doi.org/10.1145/3643688">10.1145/3643688</a></li>
    <li><a href="https://drive.google.com/file/d/1UFuq2k-xAHEllu81ROJp4t-IgMJUwafn/view">Full text</a></li>
    
    
    </ul>

  </div>

  <h2 id="abstract" class="wl">
  <span>Abstract</span>
  <a href="#abstract" aria-label="Anchor">#</a>
  <a href="#content">â†°</a>
</h2>

<p>Interpretability is a critical aspect to ensure a fair and responsible use of machine learning (ML) in high-stakes applications. Genetic programming (GP) has been used to obtain interpretable ML models  because it operates at the level of functional building blocks: if these building blocks are interpretable, there is a chance that their composition (i.e., the entire ML model) is also interpretable. However, the degree to which a model is interpretable depends on the observer. Motivated by this, we study a recently-introduced human-in-the-loop system that allows the user to steer GP&rsquo;s generation process to their preferences, which shall be online-learned by an artificial neural network (ANN). We focus on the generation of ML models as analytical functions (i.e., symbolic regression) as this is a key problem in interpretable ML, and propose a two-fold contribution. First, we devise more general representations for the ML models for the ANN to learn upon, to enable the application of the system to a wider range of problems. Second, we delve into a deeper analysis of the system&rsquo;s components. To this end, we propose an incremental experimental evaluation, aimed at (1) studying the effectiveness by which an ANN can capture the perceived interpretability for simulated users, (2) investigating how the GP&rsquo;s outcome is affected across different simulated user feedback profiles, and (3) determining whether humans participants would prefer models that were generated with or without their involvement. Our results pose clarity on pros and cons of using a human-in-the-loop approach to discover interpretable ML models with GP.</p>

</div>

        </div>
      </div>
<div id="footer">
  <div class="text-center">
    Copyright by Eric Medvet
  </div>
</div>

</div>
  </body>
</html>
