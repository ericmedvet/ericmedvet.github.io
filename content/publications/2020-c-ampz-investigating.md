{
  "title": "Investigating Similarity Metrics for Convolutional Neural Networks in the Case of Unstructured Pruning",
  "pub_year": "2020",
  "pub_type": "Conference",
  "pub_venue_name": "International Conference on Pattern Recognition Applications and Methods",
  "pub_venue_number": "9th",
  "pub_venue_acronym": "ICPRAM",
  "pub_location_city": "Valletta",
  "pub_location_country": "Malta",
  "pub_authors": "Ansuini, Alessio; Medvet, Eric; Pellegrino, Felice Andrea; Zullich, Marco",
  "pub_doi": "10.1007/978-3-030-66125-0_6",
  "pub_publisher_url": "https://link.springer.com/chapter/10.1007%2F978-3-030-66125-0_6",
  "pub_fulltext_url": "https://drive.google.com/uc?export=download&id=1Nh0Bo2XcB6vY2_IMLr7CFySv3K_Zlc9-",
  "pub_important": false
}

## Abstract
Deep Neural Networks (DNNs) are essential tools of modern science and technology. The current lack of explainability of their inner workings and of principled ways to tame their architectural complexity triggered a lot of research in recent years. There is hope that, by making sense of representations in their hidden layers, we could collect insights on how to reduce model complexity—without performance degradation—by pruning useless connections. It is natural then to ask the following question: how similar are representations in pruned and unpruned models? Even small insights could help in finding principled ways to design good lightweight models, enabling significant savings of computation, memory, time and energy. In this work, we investigate empirically this problem on a wide spectrum of similarity measures, network architectures and datasets. We find that the results depend critically on the similarity measure used and we discuss briefly the origin of these differences, concluding that further investigations are required in order to make substantial advances.
